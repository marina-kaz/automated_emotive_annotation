from pathlib import Path

import streamlit as st
import torch

from interpretable_nlp.models import (initialize_model,
                                      embeddings_mapper,
                                      MODEL_TYPE_MAPPING)
from interpretable_nlp.predict_utils import (predict_sentiment,
                                             attribution_fun,
                                             attribution_to_html,
                                             compose_annotation_file)

from transformers import AutoTokenizer


option = st.selectbox(
    '–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–∏—è',
     ('Distil RuBert Cased Conversational –æ—Ç DeepPavlov',
      'Cointegrated RuBert'))


SENTIMENT_MODEL = MODEL_TYPE_MAPPING[option]


def get_tokenizer():
    tokenizer_ = AutoTokenizer.from_pretrained(SENTIMENT_MODEL.value)
    return tokenizer_


@st.cache
def get_model():
    sentiment_model_ = initialize_model(SENTIMENT_MODEL)
    sentiment_model_.eval()
    device_ = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    sentiment_model_.to(device_)
    return sentiment_model_, device_


sentiment_model, device = get_model()
embedding = embeddings_mapper(SENTIMENT_MODEL)

tokenizer = get_tokenizer()
special_tokens = {'PAD_IDX': tokenizer.pad_token_id,
                  'SEP_IDX': tokenizer.sep_token_id,
                  'CLS_IDX': tokenizer.cls_token_id}


vocab_size = tokenizer.vocab_size
max_len = 512

markdown = st.sidebar.markdown(
    "<h2>–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –Ω–∞–Ω–µ—Å–µ–Ω–∏–µ —ç–º–æ—Ç–∏–≤–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏</h2>"
    "<h4>–ö–∞—Ç–µ–≥–æ—Ä–∏–∏ —ç–º–æ—Ü–∏–π:</h4>"
    "<ul>"
    "<li><span style='background-color:rgba(255, 255, 128, .8)'>—Ä–∞–¥–æ—Å—Ç—å</span></li>"
    "<li><span style='background-color:rgba(30,129,176, .7)'>–ø–µ—á–∞–ª—å</span></li>"
    "<li><span style='background-color:rgba(228,52,52, .7)'>–∑–ª–æ—Å—Ç—å</span></li>"
    "<li><span style='background-color:rgba(200, 114, 226, .7)'>—É–¥–∏–≤–ª–µ–Ω–∏–µ/—Å—Ç—Ä–∞—Ö</span></li>"
    "<li>–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π</span></li>"
    "</ul>",
    unsafe_allow_html=True,
)

txt = st.text_area(
    "–í–≤–µ–¥–∏—Ç–µ –∞–Ω–Ω–æ—Ç–∏—Ä—É–µ–º—ã–π —Ç–µ–∫—Å—Ç:",
    "–ö–∞–∫–∞—è –∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–∞—è –ø–æ–≥–æ–¥–∞!!))",
)

tokenized = tokenizer(txt)
sentiment_output = predict_sentiment(tokenized,
                                     model=sentiment_model,
                                     device=device)
color = {
         "—Ä–∞–¥–æ—Å—Ç—å": "rgba(255, 255, 128, .8)",
         "–ø–µ—á–∞–ª—å": "rgba(30,129,176, .7)",
         "–∑–ª–æ—Å—Ç—å": "rgba(228,52,52, .7)",
         "–Ω–µ–æ–ø—Ä–µ–¥": "rgba(200, 114, 226, .7)",
         "–Ω–µ–π—Ç—Ä": "white",
         }[sentiment_output]

emoji = {
    "—Ä–∞–¥–æ—Å—Ç—å": "üòÄ",
    "–ø–µ—á–∞–ª—å": "üò≠",
    "–∑–ª–æ—Å—Ç—å": "üò°",
    "–Ω–µ–æ–ø—Ä–µ–¥": "ü§î",
    "–Ω–µ–π—Ç—Ä": "üòê"}[sentiment_output]

st.write(
    "–≠–∫—Å–ø–ª–∏—Ü–∏—Ä—É–µ–º–∞—è —ç–º–æ—Ü–∏—è:",
    f"{emoji} <span style='background-color:{color}'>{sentiment_output}</span>",
    unsafe_allow_html=True,
)

tokens, attributions = attribution_fun(tokenized=tokenized,
                                       model=sentiment_model,
                                       embedding=embedding,
                                       device=device,
                                       special_tokens=special_tokens)
tokens = tokenizer.convert_ids_to_tokens(tokens.input_ids)[1:-1]
html = attribution_to_html(tokens=tokens, attributions=attributions, sentiment=sentiment_output)

st.write(
    "–í–∏–∑—É–∞–ª—å–Ω–∞—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è:\n\n",
    html,
    unsafe_allow_html=True,
)

annotation = compose_annotation_file(txt, tokens, attributions)
st.json(annotation)
st.download_button(
    label="–°–∫–∞—á–∞—Ç—å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é",
    file_name="annotation.json",
    mime="application/json",
    data=annotation,
)

